Lesson 6
========================================================


### Scatterplot Review
Create a scatterplot of price (y) vs carat weight (x)
Limit the x-axis and y-axis to omit the top 1% of values
```{r Scatterplot Review}
library(ggplot2)
data(diamonds)
ggplot(aes(x=carat, y=price), data=diamonds) + geom_point() +
  stat_smooth(method = 'lm') +
  coord_cartesian(xlim = c(0, quantile(diamonds$carat, .99)), ylim = c(0, quantile(diamonds$price, .99)))
```
We can see a non linear relationship, maybe it's exponential or maybe it's something else. We can see that the dispersion, or variance also increases as the carat size increases.   
We can see that the linear trend line doesn't go through the centre of the data at some key places. If we tried to use this to make predictions then we might be off for some key places. 

***


### Frances Gerety
Large supply of diamonds found in South Africa in 1870. Up until then the supply of diamonds was quite low. The De Beer Cartel controlled the prices in the international diamond market. World war 1 and the great depression saw diamond sales plummet. In 1938 the De Beer cartel inquired about using propaganda to boost the sales of diamonds. 


***

### ggpairs Function
GGPairs plots each variable against every other variable pair wise. 
```{r ggpairs Function}
# install these if necessary
install.packages('GGally') #for matrix plot
install.packages('scales')
install.packages('memisc') #summarise the regression
install.packages('lattice') #
install.packages('MASS')
install.packages('car') # to recode variables
install.packages('reshape') #reshape & wrangle data
install.packages('plyr')

# load the ggplot graphics package and the others
library(ggplot2)
library(GGally)
library(scales)
library(memisc)

# sample 10,000 diamonds from the data set
set.seed(20022012)
diamond_samp <- diamonds[sample(1:length(diamonds$price), 10000), ]
ggpairs(diamond_samp,
        axisLabels = 'internal',
  lower = list(continuous = wrap("points", shape = I('.'))),
  upper = list(combo = wrap("box", outlier.shape = I('.'))))
```
If your dataset has more than 10 columns then there will be too many plotting windows so subset if that's the case. 
In the lower triangle of the plot matrix GGPairs uses grouped histograms for qualitative - qualitative pairs and scatter plots for quantitative - quantitaive pairs. In the upper triangle it plots grouped histograms for qualitative - qualitative pairs, this time using the x instead of the y as the grouping factor. Box plots for qualitative - quantitaive pairs and it provides the correlation for quantitative - quantitaive pairs.   

What are some things you notice in the ggpairs output?
Response:
Can see a relationship between price and clarity, and between price and colour. The critical factor driving price is the size, or carat weight, of the diamond. 
The relationship between price and size is non linear. On the supply side larger pieces of diamond without significant flaws are probably haarder to find than smaller ones. This might help explain the sort of exponential looking curve. 

weight ≈ f(volume) ≈ f(x.y.z). 
This suggests that we might be intersted in the cubed root of carat weight. 
***

### The Demand of Diamonds
On the demand side customers in the market for a less expensive smaller diamond are probably more sensitive to price than more well to do buyers. Many of the less than 1 carat customers would probably never buy a diamond were it not the social norm for engagement rings. Fewer customers can afford a bigger diamond hence we shouldn't expect the market for bigger diamonds to be as competitive as the one for smaller diamonds so it makes sense that the variance, as well as the price, would increase with carat size. 
Often the distribution of any monetary variable like dollars will be very skewed and will vary over orders of magnitude. This can result from path dependence, for e.g. the rich getting richer, or multiplicitive processes like year on year inflation, or some combination of both. Hence it's a good idea to look in to compressing any such variable by putting it on a log scale. 

Create two histograms of the price variable and place them side by side on one output image.
The first plot should be a histogram of price and the second plot should transform the price variable using log10.
```{r The Demand of Diamonds}
library(gridExtra)
library(ggplot2)

p1 <- ggplot(aes(x=price), data=diamonds) + geom_histogram(binwidth = 100) + ggtitle('Price')

p2 <- ggplot(aes(x=price), data =diamonds) + geom_histogram(binwidth = 0.01) + scale_x_log10() + ggtitle('Price (log10)')

grid.arrange(p1, p2, ncol=2)
```

***

### Connecting Demand and Price Distributions
We can see that the price for diamonds is pretty heavily skewed. When you put those prices on a log10 scale they seem much better well behaved. They're much closer to the bell curve of a normal distribution. We can even see a little bit of evidence of bimodality which is consistent with our two class richer buyer, poor buyer speculation. 
***

### Scatterplot Transformation

```{r Scatterplot Transformation}

ggplot(aes(x=carat, y=price), data = diamonds) + geom_point() +
  scale_y_log10() +
  ggtitle("Price (log 10) by carat")
```
On a log scale the prices look less dispersed at the high end of carat size and price. 
We can do even better - let's use the cubed root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. 
Let's create a function to transform the carat variable:

### Create a new function to transform the carat variable

```{r cuberoot transformation}
cuberoot_trans = function() trans_new('cuberoot', transform = function(x) x^(1/3),
                                      inverse = function(x) x^3)
```

#### Use the cuberoot_trans function
```{r Use cuberoot_trans}
library(scales) # for using trans_new

ggplot(aes(carat, price), data = diamonds) + 
  geom_point() + 
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle('Price (log10) by Cube-Root of Carat')
```
After the transformation the plot almost looks linear. 
***

### Overplotting Revisited

```{r Sort and Head Tables}
head(sort(table(diamonds$price), decreasting = T))
head(sort(table(diamonds$carat), decreasting = T))
```
First line in the table is price/carat and the second line is the count for each of the values. We can see that they are really high numbers which is going to result in a substantial amount of overplotting. When you have this much data you're going to have serious over plotting even when you're plotting the variables against each other. This can obscure the density and the varsity at really key points. 
We can deal with this by making the points smaller, by jittering them and by adding transparency. 

```{r Overplotting Revisited}
ggplot(aes(carat, price), data = diamonds) + 
  geom_point(alpha = 0.5, size=0.75, position = 'jitter') + 
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle('Price (log10) by Cube-Root of Carat')
```

***

### Other Qualitative Factors
We can see what looks like an almost linear relationship between price and weight after doing some other transformations. But surely there are other factors that influence the price of a diamond. Clarity would also be an important factor. Of course, customers want diamonds of a certain minimum size so clarity shouldn't be as strong of a factor as carat weight.

***

### Price vs. Carat and Clarity

Adjust the plot to be coloured by clarity.
```{r Price vs. Carat and Clarity}
# install and load the RColorBrewer package
#install.packages('RColorBrewer')
library(RColorBrewer)

p1 <- ggplot(aes(x = carat, y = price), data = diamonds) + 
  geom_point(alpha = 0.5, size = 1, position = 'jitter', aes(color = clarity)) +
  scale_color_brewer(type = 'div',
    guide = guide_legend(title = 'Clarity', reverse = T,
    override.aes = list(alpha = 1, size = 2))) +  
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle('Price (log10) by Cube-Root of Carat and Clarity')

p1 + scale_colour_brewer("Diamond\nclarity")
```
After adding colour to our plot we can see that clarity does have a lot to do with the variance in price. Holding carat weight constant, or looking at one point of the plot, we see that diamonds with lower clarity are almost always cheaper than diamonds with better clarity.

***


### Price vs. Carat and Cut

Now let's colour by cut
```{r Price vs. Carat and Cut}
ggplot(aes(x = carat, y = price, color = cut), data = diamonds) + 
  geom_point(alpha = 0.5, size = 1, position = 'jitter') +
  scale_color_brewer(type = 'div',
                     guide = guide_legend(title = 'Cut', reverse = T,
                                          override.aes = list(alpha = 1, size = 2))) +  
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle('Price (log10) by Cube-Root of Carat and Cut')
```
We don't see much variance in price by the cut. Most of the diamonds in our dataset are ideal/premium cut. 

***


### Price vs. Carat and Color

Finally, lets colour the graph by the colour of the diamonds
```{r Price vs. Carat and Color}
ggplot(aes(x = carat, y = price, color = color), data = diamonds) + 
  geom_point(alpha = 0.5, size = 1, position = 'jitter') +
  scale_color_brewer(type = 'div',
                     guide = guide_legend(title = 'Color', reverse = FALSE, #reverse=false puts the best colour at the top of the list
                                          override.aes = list(alpha = 1, size = 2))) +  
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle('Price (log10) by Cube-Root of Colour and Cut')
```
Colour does seem to explain some of the variance in price. 

***

### Linear Models in R
lm() function: lm(x~y) x is outcome the variable and y is the explanatory variable. 
In this case we could use the following formula:
```{r}
lm(log(price) ~ carat^(1/3))
```
Price is the outcome and carat is the predictor variable. We used our domain knowledge of diamonds and carat weight to take the cube root of carat weight (volume).


***

### Building the Linear Model
The I() wrapper around each of the variables stand for as is. In this case it tells R to use the expression inside the I() function to transform a variable before using it in the regression. This is instead of instructing R to interpret these sybols as part of the formula to construct the design matrix for the regression. 
```{r Building the Linear Model}
library(memisc)

m1 <- lm(I(log(price)) ~ I(carat^(1/3)), data = diamonds)
#update prvious model to add the carat variable in the regression The real functional relationship is surely not as simple as the cubed root of carat so we add a simple linear function of carat 
m2 <- update(m1, ~ . + carat) 
m3 <- update(m2, ~ . + cut) #add cut even though we dont expect it to have much influence on price
m4 <- update(m3, ~ . + color) 
m5 <- update(m4, ~ . + clarity)
mtable(m1, m2, m3, m4, m5, sdigits = 3)
```

Notice how adding cut to our model does not help explain much of the variance
in the price of diamonds. This fits with out exploration earlier.

***

### Model Problems
Let’s put our model in a larger context. Assuming that the data is not somehow corrupted and we are not egregiously violating some of the key assumptions of linear regression (for example, violating the IID assumption by having a bunch of duplicated observations in our data set), what could be some problems with this model? What else should we think about when using this model?

Research:
(Take some time to come up with 2-4 problems for the model)

Response:
* Need to take into account inflation.
* Some diamonds can be too big to sell. When you go above 1,000 carats, it is too big.
* Miners have more advanced technology, this is why we see these large stones coming up all of a sudden. 
* From 2018 onward, as existing mines get depleted and no major new deposits come online, supply is expected to decline, falling behind expected demand growth.
* Our data is from 2008 -> 2014. Need to take in to account the recession which led to a downturn in the market. Diamond prices plummeted in 2008 due to the global recession. 
* Diamond market in China is increasing. 
* Uneven price recovery/ price increase across different weights.

***

### A Bigger, Better Data Set

Got a more complete datset of diamond prices from BigDiamonds.Rda
```{r A Bigger, Better Data Set}
#install.packages('bitops')
#install.packages('RCurl')
library('bitops')
library('RCurl')

load("BigDiamonds.rda")

 
```
We'll look at diamonds under $10,000 because these are the types of diamonds sold at most retailers and hence the kind that we care most about. Our model will also be less likely to be thrown off by outliers and the higher variance at the high end of price and carat

## Building a Model Using the Big Diamonds Data Set

Build five linear models for the diamonds data using a sample of diamonds from the diamondsbig data set.
Be sure to make use of the same variables (logprice, carat, etc.) and model names (m1, m2, m3, m4, m5)
```{r Building a Model Using the Big Diamonds Data Set}
diamondsbig$logprice = log(diamondsbig$price)
m1 <- lm(logprice ~ I(carat^(1/3)), data = diamondsbig[diamondsbig$price<10000 &
                                                            diamondsbig$cert == 'GIA',])
m2 <- update(m1, ~ . + carat) 
m3 <- update(m2, ~ . + cut) 
m4 <- update(m3, ~ . + color) 
m5 <- update(m4, ~ . + clarity)
mtable(m1, m2, m3, m4, m5, sdigits = 3) 
```

***

## Predictions
Let's use our model to make a prediction. We need to exponentiate the results of our model since we took the log of price. Let's take a look at an example from BlueNile. We'll use the full model, m5, to prdeict the value of the diamond. 
Example Diamond from BlueNile:
Round 1.00 Very Good I VS1 $5,601

```{r}
#Be sure you’ve loaded the library memisc and have m5 saved as an object in your workspace.
thisDiamond = data.frame(carat = 1.00, cut = "V.Good",
                         color = "I", clarity="VS1")
modelEstimate = predict(m5, newdata = thisDiamond,
                        interval="prediction", level = .95)
exp(modelEstimate)

```

Evaluate how well the model predicts the BlueNile diamond's price. Think about the fitted point estimate as well as the 95% CI.

The results yield an expected value for price given the characteristics of our diamond and the upper and lower bounds of a 95% confidence interval. Note: because this is a linear model prediction is just multiplying each model coefficient by each value in our data. 
It turns out that this diamond is a bit pricier than the expected value under the full model, though it is by no means outside of the 95% confidence interval. 
While this model might give you a sense of whether your diamond is a rip off against diamondse.info diamonds, it's not clear that diamondse.info should be regarded as the universal source of truth about whther the price of a diamond is reasonable. Nonetheless, to have the expected price diamondse.info within 95% CI is a lot more information than we had about the price we should be willing to pay for a diamond. 

The prediction interval here may be slightly conservative, as the model errors are heteroskedastic over carat (and hence price) even after our log and cube-root transformations.


***